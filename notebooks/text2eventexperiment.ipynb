{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd527ce-2791-4f2d-b56e-02b9d0b45a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93163c85-6c7d-4192-837c-0040c3b2c20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 16:02:39.104915: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-30 16:02:39.248190: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-30 16:02:40.186107: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.abspath(os.getcwd()), \"..\"))\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from typing import get_args\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from certainty import load_file, CACHE_DIR, seed_everything, RANDOM_SEED, EventType, extract_events, TRAIN_FILENAME, TEST_FILENAME, DEV_FILENAME, load_events\n",
    "OUTPUT_DIR = \"../models/blabla\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1346e9be-8c64-4d59-8f64-70bfbee59251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "import transformers\n",
    "from filelock import FileLock\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    MBartTokenizer,\n",
    "    default_data_collator,\n",
    "    set_seed\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddbf9e65-2f17-4ce6-8c3c-370ff97ec9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactualitySchema:\n",
    "    def __init__(self, type_list):\n",
    "        self.type_list = type_list\n",
    "        self.role_list = []\n",
    "        self.type_role_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bab1fd2-93c7-458f-86c7-8362d2e497dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"factuality\"\n",
    "text_column = \"text\"\n",
    "summary_column = \"factuality\"\n",
    "max_source_length = 256\n",
    "max_target_length = 128\n",
    "pad_to_max_length = True\n",
    "source_lang = \"en\"\n",
    "ignore_pad_token_for_loss = True\n",
    "source_prefix = \"factuality:\"\n",
    "decoding_format = \"tree\"\n",
    "model_name='t5-small'\n",
    "prefix=\"Pairs of Factualities and triggers: \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0a3fd56-a7b3-4296-9e94-3e9a5788d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_file('en_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b589595-9a38-47f7-86b8-d5a888bb977e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent_id': 'bc/CNN_IP_20030329.1600.02/001',\n",
       " 'text': 'It was in northern Iraq today that an eight artillery round hit the site occupied by Kurdish fighters near Chamchamal',\n",
       " 'events': [{'event_type': 'Attack',\n",
       "   'event_polarity': 'Positive',\n",
       "   'event_genericity': 'Specific',\n",
       "   'event_modality': 'Asserted',\n",
       "   'trigger': [['hit'], ['60:63']],\n",
       "   'arguments': [[['northern Iraq'], ['10:23'], 'Place'],\n",
       "    [['today'], ['24:29'], 'Time-Within'],\n",
       "    [['an eight artillery round'], ['35:59'], 'Instrument'],\n",
       "    [['the site occupied by Kurdish fighters near Chamchamal'],\n",
       "     ['64:117'],\n",
       "     'Target']]}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.DataFrame(train).drop_duplicates('text').drop_duplicates('events').to_dict(\"records\")\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0dd6feb-ba85-46d6-9d9a-acce1474e18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate: 11\n",
      "['war', 'war']\n",
      "{'war'}\n",
      "That Europe is against the war on humanitarian and moral grounds , but the U.S. is for the war because it wants to profit for the oil companies\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(train):\n",
    "    triggers = list(map(lambda e: e['trigger'][0][0], sample['events']))\n",
    "    trigger_set = set(triggers)\n",
    "\n",
    "    if len(triggers) != len(trigger_set):\n",
    "        print(\"Duplicate: \" + str(i))\n",
    "        print(triggers)\n",
    "        print(trigger_set)\n",
    "        print(sample['text'])\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90dcdfe5-2b32-4c72-bdf9-3da20af155c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = []\n",
    "for sample in train:\n",
    "    pairs = \"(\"\n",
    "    for i, event in enumerate(sample['events']):\n",
    "        pair = \"(\" + event['event_modality'] + \" \" + event['trigger'][0][0] + \")\"\n",
    "        if i > 0:\n",
    "            pairs += \" \"\n",
    "        pairs += str(pair)\n",
    "    pairs += \")\"\n",
    "    new.append({'factuality': pairs,\n",
    "                 'text': sample['text']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "803e3321-edb4-4949-9619-9cc862043044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'factuality': '((Asserted pummeled) (Asserted retreat))',\n",
       " 'text': \"That 's because coalition fighter jets pummeled this Iraqi position on the hills above Chamchamal and Iraqi troops made a hasty retreat\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f0ca42a-b077-44f4-96fe-ffe49823622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(pd.DataFrame(new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "376c2380-0518-46a2-96ee-8c449765b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    model_name, \n",
    "    cache_dir=CACHE_DIR, \n",
    "    #local_files_only=True, \n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f58af57e-d3fc-4c29-aa4a-a76b597e9107",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.max_length = max_target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa093d86-6ed0-4edf-8f08-f690fac2e5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        use_fast=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22280bb4-c19f-42ba-8167-cfbd468a9eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove_token_list = list()\n",
    "if tokenizer.bos_token:\n",
    "    to_remove_token_list += [tokenizer.bos_token]\n",
    "if tokenizer.eos_token:\n",
    "    to_remove_token_list += [tokenizer.eos_token]\n",
    "if tokenizer.pad_token:\n",
    "    to_remove_token_list += [tokenizer.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "873c66db-270f-4ae1-95d3-44b54e0ca644",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        cache_dir=CACHE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b702afce-8d37-41e1-805b-1c7bd1fcd920",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model.config.decoder_start_token_id is None and isinstance(tokenizer, MBartTokenizer):\n",
    "    model.config.decoder_start_token_id = tokenizer.lang_code_to_id['en']\n",
    "if model.config.decoder_start_token_id is None:\n",
    "    raise ValueError(\n",
    "        \"Make sure that `config.decoder_start_token_id` is correctly defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bb817ce-9aa5-4e69-8d29-116a126f1c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98fb901d-2717-4ad9-9e4d-b6f86ee0b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_type_schema = FactualitySchema(['Asserted', 'Other'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5f1f956-c124-4d61-b127-f6171ef13807",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = \"max_length\" if pad_to_max_length else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54c14065-1cc5-4692-806e-ebe5b379c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples[text_column]\n",
    "    targets = examples[summary_column]\n",
    "    inputs = [prefix + inp for inp in inputs]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=1024, padding=padding, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\" and ignore_pad_token_for_loss:\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5712ed5e-de8a-4bb7-b152-296c31e11f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'factuality': '((Asserted pummeled) (Asserted retreat))', 'text': \"That 's because coalition fighter jets pummeled this Iraqi position on the hills above Chamchamal and Iraqi troops made a hasty retreat\"}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c1d490a-ea86-4c32-97f4-14bc55316e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c76b20ea-ff7c-4dd4-bfe0-8786754d9e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac08a2651f1743ae808d429aea93511d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3246 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peder/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            remove_columns=train_dataset.column_names\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32b205b8-e1e2-4fe9-8b04-1a871874d96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(\"pt\", columns=[\"input_ids\", \"labels\", \"attention_mask\"], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "613b82fa-6afd-4b18-b1f3-5ad2a89eccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pad_token_id = - \\\n",
    "    100 if True else tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "992abe48-dbe7-4877-9bd8-74a44ab5b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer,\n",
    "            model=model,\n",
    "            label_pad_token_id=label_pad_token_id,\n",
    "            pad_to_multiple_of=8 if True else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b399105-43ed-4fa8-80b8-1eb3f0e47f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282\n",
      "  7\n",
      "    49\n",
      "      1054\n",
      "        <end>\n",
      "933\n",
      "  188\n",
      "    7\n",
      "      7\n",
      "        49\n",
      "          1054\n",
      "            <end>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from typing import Dict\n",
    "\n",
    "def get_label_name_tree(label_name_list, tokenizer, end_symbol='<end>'):\n",
    "    sub_token_tree = dict()\n",
    "\n",
    "    label_tree = dict()\n",
    "    for typename in label_name_list:\n",
    "        after_tokenized = tokenizer.encode(typename, add_special_tokens=False)\n",
    "        label_tree[typename] = after_tokenized\n",
    "\n",
    "    for _, sub_label_seq in label_tree.items():\n",
    "        parent = sub_token_tree\n",
    "        for value in sub_label_seq:\n",
    "            if value not in parent:\n",
    "                parent[value] = dict()\n",
    "            parent = parent[value]\n",
    "\n",
    "        parent[end_symbol] = None  # Mark end of valid sequence\n",
    "\n",
    "    return sub_token_tree\n",
    "\n",
    "\n",
    "class PrefixTree:\n",
    "    def __init__(self, label_name_list, tokenizer, end_symbol='<end>'):\n",
    "        self.label_name_list = label_name_list\n",
    "        self._tokenizer = tokenizer\n",
    "        self.label_name_tree = get_label_name_tree(label_name_list, tokenizer, end_symbol)\n",
    "        self._end_symbol = end_symbol\n",
    "\n",
    "    def is_end_of_tree(self, tree: Dict):\n",
    "        return len(tree) == 1 and self._end_symbol in tree\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    factuality_labels = [\"Asserted\", \"NotAsserted\"]  # Replace event types with factuality labels\n",
    "\n",
    "    test_tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "\n",
    "    factuality_tree = get_label_name_tree(factuality_labels, test_tokenizer)\n",
    "\n",
    "    # Function to print the tree structure (if needed)\n",
    "    def print_tree(tree, indent=0):\n",
    "        for key, value in tree.items():\n",
    "            print(\"  \" * indent + str(key))\n",
    "            if isinstance(value, dict):\n",
    "                print_tree(value, indent + 1)\n",
    "\n",
    "    print_tree(factuality_tree)  # Visualize the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc5c8cd4-08ef-45da-8dce-6a6c47945465",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Union, List, Callable, Dict, Tuple, Any, Optional\n",
    "\n",
    "def match_sublist(the_list, to_match):\n",
    "    \"\"\"\n",
    "\n",
    "    :param the_list: [1, 2, 3, 4, 5, 6, 1, 2, 4, 5]\n",
    "    :param to_match: [1, 2]\n",
    "    :return:\n",
    "        [(0, 1), (6, 7)]\n",
    "    \"\"\"\n",
    "    len_to_match = len(to_match)\n",
    "    matched_list = list()\n",
    "    for index in range(len(the_list) - len_to_match + 1):\n",
    "        if to_match == the_list[index:index + len_to_match]:\n",
    "            matched_list += [(index, index + len_to_match - 1)]\n",
    "    return matched_list\n",
    "\n",
    "\n",
    "def find_bracket_position(generated_text, _type_start, _type_end):\n",
    "    bracket_position = {_type_start: list(), _type_end: list()}\n",
    "    for index, char in enumerate(generated_text):\n",
    "        if char in bracket_position:\n",
    "            bracket_position[char] += [index]\n",
    "    return bracket_position\n",
    "\n",
    "\n",
    "def generated_search_src_sequence(generated, src_sequence, end_sequence_search_tokens=None):\n",
    "\n",
    "    if len(generated) == 0:\n",
    "        # It has not been generated yet. All SRC are valid.\n",
    "        return src_sequence\n",
    "\n",
    "    matched_tuples = match_sublist(the_list=src_sequence, to_match=generated)\n",
    "\n",
    "    valid_token = list()\n",
    "    for _, end in matched_tuples:\n",
    "        next_index = end + 1\n",
    "        if next_index < len(src_sequence):\n",
    "            valid_token += [src_sequence[next_index]]\n",
    "\n",
    "    if end_sequence_search_tokens:\n",
    "        valid_token += end_sequence_search_tokens\n",
    "\n",
    "    return valid_token\n",
    "\n",
    "\n",
    "class ConstraintDecoder:\n",
    "    def __init__(self, tokenizer, source_prefix):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_prefix = source_prefix\n",
    "        self.source_prefix_tokenized = tokenizer.encode(source_prefix,\n",
    "                                                        add_special_tokens=False) if source_prefix else []\n",
    "\n",
    "    def get_state_valid_tokens(self, src_sentence: List[str], tgt_generated: List[str]) -> List[str]:\n",
    "        pass\n",
    "\n",
    "    def constraint_decoding(self, src_sentence, tgt_generated):\n",
    "        if self.source_prefix_tokenized:\n",
    "            # Remove Source Prefix for Generation\n",
    "            src_sentence = src_sentence[len(self.source_prefix_tokenized):]\n",
    "\n",
    "        valid_token_ids = self.get_state_valid_tokens(\n",
    "            src_sentence.tolist(),\n",
    "            tgt_generated.tolist()\n",
    "        )\n",
    "\n",
    "        # return self.tokenizer.convert_tokens_to_ids(valid_tokens)\n",
    "        return valid_token_ids\n",
    "\n",
    "\n",
    "\n",
    "class TreeConstraintDecoder(ConstraintDecoder):\n",
    "    def __init__(self, tokenizer, type_schema, *args, **kwargs):\n",
    "        super().__init__(tokenizer, *args, **kwargs)\n",
    "        self.tree_end = '<tree-end>'\n",
    "        self.type_tree = get_label_name_tree(\n",
    "            type_schema.type_list, self.tokenizer, end_symbol=self.tree_end)\n",
    "        self.role_tree = get_label_name_tree(\n",
    "            type_schema.role_list, self.tokenizer, end_symbol=self.tree_end)\n",
    "        self.type_start = self.tokenizer.convert_tokens_to_ids([\"(\"])[0]\n",
    "        self.type_end = self.tokenizer.convert_tokens_to_ids([\")\"])[0]\n",
    "\n",
    "    def check_state(self, tgt_generated):\n",
    "        if tgt_generated[-1] == self.tokenizer.pad_token_id:\n",
    "            return 'start', -1\n",
    "\n",
    "        special_token_set = {self.type_start, self.type_end}\n",
    "        special_index_token = list(\n",
    "            filter(lambda x: x[1] in special_token_set, list(enumerate(tgt_generated))))\n",
    "\n",
    "        last_special_index, last_special_token = special_index_token[-1]\n",
    "\n",
    "        if len(special_index_token) == 1:\n",
    "            if last_special_token != self.type_start:\n",
    "                return 'error', 0\n",
    "\n",
    "        bracket_position = find_bracket_position(\n",
    "            tgt_generated, _type_start=self.type_start, _type_end=self.type_end)\n",
    "        start_number, end_number = len(bracket_position[self.type_start]), len(\n",
    "            bracket_position[self.type_end])\n",
    "\n",
    "        if start_number == end_number:\n",
    "            return 'end_generate', -1\n",
    "        if start_number == end_number + 1:\n",
    "            state = 'start_first_generation'\n",
    "        elif start_number == end_number + 2:\n",
    "            state = 'generate_trigger'\n",
    "        elif start_number == end_number + 3:\n",
    "            state = 'generate_role'\n",
    "        else:\n",
    "            state = 'error'\n",
    "        return state, last_special_index\n",
    "\n",
    "    def search_prefix_tree_and_sequence(self, generated: List[str], prefix_tree: Dict, src_sentence: List[str],\n",
    "                                        end_sequence_search_tokens: List[str] = None):\n",
    "        \"\"\"\n",
    "        Generate Type Name + Text Span\n",
    "        :param generated:\n",
    "        :param prefix_tree:\n",
    "        :param src_sentence:\n",
    "        :param end_sequence_search_tokens:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        tree = prefix_tree\n",
    "        for index, token in enumerate(generated):\n",
    "            tree = tree[token]\n",
    "            is_tree_end = len(tree) == 1 and self.tree_end in tree\n",
    "\n",
    "            if is_tree_end:\n",
    "                valid_token = generated_search_src_sequence(\n",
    "                    generated=generated[index + 1:],\n",
    "                    src_sequence=src_sentence,\n",
    "                    end_sequence_search_tokens=end_sequence_search_tokens,\n",
    "                )\n",
    "                return valid_token\n",
    "\n",
    "            if self.tree_end in tree:\n",
    "                try:\n",
    "                    valid_token = generated_search_src_sequence(\n",
    "                        generated=generated[index + 1:],\n",
    "                        src_sequence=src_sentence,\n",
    "                        end_sequence_search_tokens=end_sequence_search_tokens,\n",
    "                    )\n",
    "                    return valid_token\n",
    "                except IndexError:\n",
    "                    # Still search tree\n",
    "                    continue\n",
    "\n",
    "        valid_token = list(tree.keys())\n",
    "        return valid_token\n",
    "\n",
    "    def get_state_valid_tokens(self, src_sentence, tgt_generated):\n",
    "        \"\"\"\n",
    "\n",
    "        :param src_sentence:\n",
    "        :param tgt_generated:\n",
    "        :return:\n",
    "            List[str], valid token list\n",
    "        \"\"\"\n",
    "        if self.tokenizer.eos_token_id in src_sentence:\n",
    "            src_sentence = src_sentence[:src_sentence.index(\n",
    "                self.tokenizer.eos_token_id)]\n",
    "\n",
    "        state, index = self.check_state(tgt_generated)\n",
    "\n",
    "\n",
    "        if state == 'error':\n",
    "            print(\"Error:\")\n",
    "            print(\"Src:\", src_sentence)\n",
    "            print(\"Tgt:\", tgt_generated)\n",
    "            valid_tokens = [self.tokenizer.eos_token_id]\n",
    "\n",
    "        elif state == 'start':\n",
    "            valid_tokens = [self.type_start]\n",
    "\n",
    "        elif state == 'start_first_generation':\n",
    "            valid_tokens = [self.type_start, self.type_end]\n",
    "\n",
    "        elif state == 'generate_trigger':\n",
    "\n",
    "            if tgt_generated[-1] == self.type_start:\n",
    "                # Start Event Label\n",
    "                return list(self.type_tree.keys())\n",
    "\n",
    "            elif tgt_generated[-1] == self.type_end:\n",
    "                # EVENT_TYPE_LEFT: Start a new role\n",
    "                # EVENT_TYPE_RIGHT: End this event\n",
    "                return [self.type_start, self.type_end]\n",
    "            else:\n",
    "                valid_tokens = self.search_prefix_tree_and_sequence(\n",
    "                    generated=tgt_generated[index + 1:],\n",
    "                    prefix_tree=self.type_tree,\n",
    "                    src_sentence=src_sentence,\n",
    "                    end_sequence_search_tokens=[self.type_start, self.type_end]\n",
    "                )\n",
    "\n",
    "        elif state == 'generate_role':\n",
    "\n",
    "            if tgt_generated[-1] == self.type_start:\n",
    "                # Start Role Label\n",
    "                return list(self.role_tree.keys())\n",
    "\n",
    "            generated = tgt_generated[index + 1:]\n",
    "            valid_tokens = self.search_prefix_tree_and_sequence(\n",
    "                generated=generated,\n",
    "                prefix_tree=self.role_tree,\n",
    "                src_sentence=src_sentence,\n",
    "                end_sequence_search_tokens=[self.type_end]\n",
    "            )\n",
    "\n",
    "        elif state == 'end_generate':\n",
    "            valid_tokens = [self.tokenizer.eos_token_id]\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'State `%s` for %s is not implemented.' % (state, self.__class__))\n",
    "\n",
    "        return valid_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f5fcb42-c551-4c08-a44e-43c7203634d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constraint_decoder(tokenizer, type_schema, decoding_schema, source_prefix=None):\n",
    "    return TreeConstraintDecoder(tokenizer=tokenizer, type_schema=type_schema, source_prefix=source_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fcc36f3c-6a97-43a1-aec6-1572ac28a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class ConstraintSeq2SeqTrainingArguments(Seq2SeqTrainingArguments):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        constraint_decoding (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Whether to use Constraint Decoding\n",
    "        structure_weight (:obj:`float`, `optional`, defaults to :obj:`None`):\n",
    "    \"\"\"\n",
    "    constraint_decoding: bool = field(default=False, metadata={\"help\": \"Whether to Constraint Decoding or not.\"})\n",
    "    label_smoothing_sum: bool = field(default=False,\n",
    "                                      metadata={\"help\": \"Whether to use sum token loss for label smoothing\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bea59d78-0519-4bdf-b0ca-ade4ca5ac534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = ConstraintSeq2SeqTrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        constraint_decoding=True,\n",
    "        num_train_epochs=10,\n",
    "        predict_with_generate=True,\n",
    "        label_smoothing_factor=0.1,  # Correct parameter\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a469f934-256e-4ffd-b961-6989d6818267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    PreTrainedTokenizer,\n",
    "    EvalPrediction,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    default_data_collator,\n",
    ")\n",
    "import torch.nn as nn\n",
    "from typing import Union, List, Callable, Dict, Tuple, Any, Optional\n",
    "\n",
    "\n",
    "class ConstraintSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def __init__(self, decoding_type_schema=None, decoding_format='tree', source_prefix=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.decoding_format = decoding_format\n",
    "        self.decoding_type_schema = decoding_type_schema\n",
    "        print(self.decoding_format)\n",
    "        print(self.decoding_type_schema)\n",
    "\n",
    "        # Label smoothing by sum token loss, different from different Label smootheing\n",
    "\n",
    "        if self.args.label_smoothing_sum and self.args.label_smoothing_factor != 0:\n",
    "            self.label_smoother = SumLabelSmoother(epsilon=self.args.label_smoothing_factor)\n",
    "            print('Using %s' % self.label_smoother)\n",
    "        elif self.args.label_smoothing_factor != 0:\n",
    "            print('Using %s' % self.label_smoother)\n",
    "        else:\n",
    "            self.label_smoother = None\n",
    "\n",
    "        print(self.label_smoother)\n",
    "        if self.args.constraint_decoding:\n",
    "            self.constraint_decoder = get_constraint_decoder(tokenizer=self.tokenizer,\n",
    "                                                             type_schema=self.decoding_type_schema,\n",
    "                                                             decoding_schema=self.decoding_format,\n",
    "                                                             source_prefix=source_prefix)\n",
    "            print(self.constraint_decoder)\n",
    "        else:\n",
    "            self.constraint_decoder = None\n",
    "        print(\"Trainer initialized! Training will use constraint decoding?\", self.args.constraint_decoding)\n",
    "\n",
    "    def prediction_step(\n",
    "            self,\n",
    "            model: nn.Module,\n",
    "            inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "            prediction_loss_only: bool,\n",
    "            ignore_keys: Optional[List[str]] = None,\n",
    "    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Perform an evaluation step on :obj:`model` using obj:`inputs`.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`nn.Module`):\n",
    "                The model to evaluate.\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
    "            prediction_loss_only (:obj:`bool`):\n",
    "                Whether or not to return the loss only.\n",
    "\n",
    "        Return:\n",
    "            Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss, logits and\n",
    "            labels (each being optional).\n",
    "        \"\"\"\n",
    "\n",
    "        def prefix_allowed_tokens_fn(batch_id, sent):\n",
    "            # print(self.tokenizer.convert_ids_to_tokens(inputs['labels'][batch_id]))\n",
    "            src_sentence = inputs['input_ids'][batch_id]\n",
    "            return self.constraint_decoder.constraint_decoding(src_sentence=src_sentence,\n",
    "                                                               tgt_generated=sent)\n",
    "\n",
    "        if not self.args.predict_with_generate or prediction_loss_only:\n",
    "            return super().prediction_step(\n",
    "                model=model,\n",
    "                inputs=inputs,\n",
    "                prediction_loss_only=prediction_loss_only,\n",
    "                ignore_keys=ignore_keys,\n",
    "                prefix_allowed_tokens_fn=prefix_allowed_tokens_fn if self.constraint_decoder else None,\n",
    "            )\n",
    "\n",
    "        has_labels = \"labels\" in inputs\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        gen_kwargs = {\n",
    "            \"max_length\": self.model.config.max_length,\n",
    "            \"num_beams\": self.model.config.num_beams,\n",
    "            \"prefix_allowed_tokens_fn\": prefix_allowed_tokens_fn if self.constraint_decoder else None,\n",
    "        }\n",
    "\n",
    "        generated_tokens = self.model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            **gen_kwargs,\n",
    "        )\n",
    "\n",
    "        # in case the batch is shorter than max length, the output should be padded\n",
    "        if generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n",
    "            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            if has_labels:\n",
    "                if self.label_smoother is not None:\n",
    "                    loss = self.label_smoother(outputs, inputs[\"labels\"]).mean().detach()\n",
    "                else:\n",
    "                    loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n",
    "            else:\n",
    "                loss = None\n",
    "\n",
    "        if self.args.prediction_loss_only:\n",
    "            return loss, None, None\n",
    "\n",
    "        labels = inputs[\"labels\"]\n",
    "        if labels.shape[-1] < gen_kwargs[\"max_length\"]:\n",
    "            labels = self._pad_tensors_to_max_len(labels, gen_kwargs[\"max_length\"])\n",
    "\n",
    "        return loss, generated_tokens, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "85ce2d01-24a4-4f6a-9ac4-d781d851a581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7357/2536087298.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `ConstraintSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree\n",
      "<__main__.FactualitySchema object at 0x75873977caf0>\n",
      "Using LabelSmoother(epsilon=0.1, ignore_index=-100)\n",
      "LabelSmoother(epsilon=0.1, ignore_index=-100)\n",
      "<__main__.TreeConstraintDecoder object at 0x7586042877c0>\n",
      "Trainer initialized! Training will use constraint decoding? True\n"
     ]
    }
   ],
   "source": [
    "trainer = ConstraintSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset.select(range(200)),\n",
    "    eval_dataset=train_dataset.select(range(50)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    decoding_type_schema=decoding_type_schema,\n",
    "    decoding_format='tree',\n",
    "    source_prefix=prefix,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "171efce9-96af-4906-bcbd-8d3375c0f0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 04:56, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Trigger Precision</th>\n",
       "      <th>Trigger Recall</th>\n",
       "      <th>Trigger F1</th>\n",
       "      <th>Discovered Other Recall</th>\n",
       "      <th>Discovered Other Precision</th>\n",
       "      <th>Discovered Other F1</th>\n",
       "      <th>Discovered Asserted Recall</th>\n",
       "      <th>Discovered Asserted Precision</th>\n",
       "      <th>Discovered Asserted F1</th>\n",
       "      <th>Asserted Precision</th>\n",
       "      <th>Asserted Recall</th>\n",
       "      <th>Asserted F1</th>\n",
       "      <th>Other Precision</th>\n",
       "      <th>Other Recall</th>\n",
       "      <th>Other F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.585396</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.763359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.649573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.585131</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.578724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.907563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.558194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.643700</td>\n",
       "      <td>1.556314</td>\n",
       "      <td>0.797297</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.848921</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.876190</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.736000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.643700</td>\n",
       "      <td>1.558743</td>\n",
       "      <td>0.701149</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>0.802632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.880734</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.643700</td>\n",
       "      <td>1.550248</td>\n",
       "      <td>0.701149</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>0.802632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.880734</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.643700</td>\n",
       "      <td>1.543612</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.861314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.876190</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.747967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.643700</td>\n",
       "      <td>1.547158</td>\n",
       "      <td>0.808219</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.855072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.876190</td>\n",
       "      <td>0.630137</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.644500</td>\n",
       "      <td>1.548300</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.863309</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.878505</td>\n",
       "      <td>0.635135</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=1.6441085205078125, metrics={'train_runtime': 296.9471, 'train_samples_per_second': 6.735, 'train_steps_per_second': 3.368, 'total_flos': 541367205888000.0, 'train_loss': 1.6441085205078125, 'epoch': 10.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "607c7c8f-b970-457a-93e4-332266e4058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_sample(sample):\n",
    "    pairs = re.findall(r'\\(?\\s*(\\w+)\\s+((?:\\w+\\s*)+?)(?=\\s*\\)?(?:\\s*\\(|\\)\\))|\\s*\\)\\))', sample)\n",
    "    return [(typ, content.strip()) for typ, content in pairs]\n",
    "\n",
    "def get_word_fact(parsed, is_true):\n",
    "    word_fact = {}\n",
    "    for sample in parsed:\n",
    "        factuality = sample[0]\n",
    "        word = sample[1]\n",
    "        if is_true:\n",
    "            polarity = sample[2]\n",
    "            genericity = sample[3]\n",
    "            e_type = sample[4]\n",
    "            text = sample[5]\n",
    "        if word in word_fact:\n",
    "            if is_true:\n",
    "                word_fact[word].append((factuality, polarity, genericity, e_type, text))\n",
    "            else:\n",
    "                word_fact[word].append((factuality,))\n",
    "        else:\n",
    "            if is_true:\n",
    "                word_fact[word] = [(factuality, polarity, genericity, e_type, text)]\n",
    "            else:\n",
    "                word_fact[word] = [(factuality,)]\n",
    "    return word_fact\n",
    "    \n",
    "eval_set = train\n",
    "df = None\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(\n",
    "        preds, skip_special_tokens=False)\n",
    "    if True:\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(\n",
    "        labels, skip_special_tokens=False)\n",
    "\n",
    "    def clean_str(x_str):\n",
    "        for to_remove_token in to_remove_token_list:\n",
    "            x_str = x_str.replace(to_remove_token, '')\n",
    "        return x_str.strip()\n",
    "\n",
    "    decoded_preds = [clean_str(x) for x in decoded_preds]\n",
    "    decoded_labels = [clean_str(x) for x in decoded_labels]\n",
    "\n",
    "    parsed_pred = list(map(parse_sample, decoded_preds))\n",
    "    parsed_true = []\n",
    "    for sample in eval_set[:len(parsed_pred)]:\n",
    "        parsed_true.append([(event['event_modality'], \n",
    "                             event['trigger'][0][0], \n",
    "                             event['event_polarity'], \n",
    "                             event['event_genericity'], \n",
    "                             event['event_type'],\n",
    "                     sample['text']) for event in sample[\"events\"]])\n",
    "        \n",
    "    trues = [get_word_fact(sample, True) for sample in parsed_true]\n",
    "    preds = [get_word_fact(sample, False) for sample in parsed_pred]\n",
    "    spurious = []\n",
    "    undiscovered = []\n",
    "    discovered = []\n",
    "    for true_wf, pred_wf in zip(trues, preds):\n",
    "        for key, value in pred_wf.items():\n",
    "            if key not in true_wf:\n",
    "                spurious += [\n",
    "                    {\"true\": \"Other\" if el[0] == 'Asserted' else \"Asserted\",\n",
    "                     \"pred\": el[0],\n",
    "                     \"trigger\": key,\n",
    "                     \"label\": \"spurious\"\n",
    "                    }\n",
    "                    for el in value\n",
    "                ]\n",
    "            elif len(value) > len(true_wf[key]):\n",
    "                # Key is in true, but length is longer for pred, we then have more spurious events\n",
    "                spurious += [\n",
    "                    {\"true\": \"Other\" if el[0] == 'Asserted' else \"Asserted\",\n",
    "                     \"pred\": el[0],\n",
    "                     \"trigger\": key,\n",
    "                     \"label\": \"spurious\"\n",
    "                    }\n",
    "                    for el in value[len(true_wf[key]):]\n",
    "                ]\n",
    "            else:\n",
    "                #key is in true, but length is shorter than for true, we have undiscovered events\n",
    "                undiscovered += [\n",
    "                    {\"true\": el[0],\n",
    "                     \"pred\": \"Other\" if el[0] == 'Asserted' else \"Asserted\",\n",
    "                     \"trigger\": key,\n",
    "                     \"polarity\": el[1],\n",
    "                     \"genericity\": el[2],\n",
    "                     \"type\": el[3],\n",
    "                     \"text\": el[4],\n",
    "                     \"label\": \"undiscovered\"\n",
    "                    }\n",
    "                     for el in true_wf[key][len(value):]\n",
    "                ]\n",
    "            \n",
    "        for key, value in true_wf.items():\n",
    "            if key in pred_wf:\n",
    "                discovered += [\n",
    "                    {\"true\": t[0],\n",
    "                     \"pred\": p[0],\n",
    "                     \"polarity\": t[1],\n",
    "                     \"genericity\": t[2],\n",
    "                     \"type\": t[3],\n",
    "                     \"trigger\": key,\n",
    "                     \"label\": \"discovered\",\n",
    "                     \"text\": t[4]}\n",
    "                    for t, p in zip(value, pred_wf[key])\n",
    "                ]\n",
    "            else:\n",
    "                undiscovered += [\n",
    "                     {\"true\": t[0],\n",
    "                      \"pred\": \"Other\" if t[0] == 'Asserted' else \"Asserted\",\n",
    "                      \"polarity\": t[1],\n",
    "                      \"genericity\": t[2],\n",
    "                      \"type\": t[3],\n",
    "                      \"trigger\": key,\n",
    "                      \"label\": \"undiscovered\",\n",
    "                      \"text\": t[4]}\n",
    "                    for t in value\n",
    "                ]\n",
    "    df = pd.DataFrame(spurious + discovered + undiscovered)\n",
    "    trigger_fp = len(spurious)\n",
    "    trigger_fn = len(undiscovered)\n",
    "    trigger_tp = len(discovered)\n",
    "    trigger_precision = trigger_tp / (trigger_tp + trigger_fp)\n",
    "    trigger_recall = trigger_tp / (trigger_tp + trigger_fn)\n",
    "    trigger_f1 = (2*trigger_precision*trigger_recall) / (trigger_precision + trigger_recall)\n",
    "\n",
    "    discovered_other_fp = len(df[(df['label'] == 'discovered') & (df['true'] == 'Asserted') & (df['pred'] == 'Other')])\n",
    "    discovered_other_fn = len(df[(df['label'] == 'discovered') & (df['true'] == 'Other') & (df['pred'] == 'Asserted')])\n",
    "    discovered_other_tp = len(df[(df['label'] == 'discovered') & (df['true'] == 'Other') & (df['pred'] == 'Other')])\n",
    "    \n",
    "    discovered_asserted_fp = len(df[(df['label'] == 'discovered') & (df['true'] == 'Other') & (df['pred'] == 'Asserted')])\n",
    "    discovered_asserted_fn = len(df[(df['label'] == 'discovered') & (df['true'] == 'Asserted') & (df['pred'] == 'Other')])\n",
    "    discovered_asserted_tp = len(df[(df['label'] == 'discovered') & (df['true'] == 'Asserted') & (df['pred'] == 'Asserted')])\n",
    "\n",
    "    discovered_other_precision = discovered_other_tp / (discovered_other_tp + discovered_other_fp) if discovered_other_tp + discovered_other_fp > 0 else 0.0\n",
    "    discovered_other_recall = discovered_other_tp / (discovered_other_tp + discovered_other_fn) if discovered_other_tp + discovered_other_fn > 0 else 0.0\n",
    "    discovered_other_f1 = (2*discovered_other_precision * discovered_other_recall)/(discovered_other_recall + discovered_other_precision) if (discovered_other_recall + discovered_other_precision) else 0.0\n",
    "    \n",
    "    discovered_asserted_precision = discovered_asserted_tp / (discovered_asserted_tp + discovered_asserted_fp) if (discovered_asserted_tp + discovered_asserted_fp) > 0 else 0.0\n",
    "    discovered_asserted_recall = discovered_asserted_tp / (discovered_asserted_tp + discovered_asserted_fn) if (discovered_asserted_tp + discovered_asserted_fn) > 0 else 0.0\n",
    "    discovered_asserted_f1 = (2*discovered_asserted_precision * discovered_asserted_recall)/(discovered_asserted_recall + discovered_asserted_precision) if (discovered_asserted_recall + discovered_asserted_precision) > 0 else 0.0\n",
    "\n",
    "    asserted_fp = len(df[(df['label'] == 'spurious') & (df['pred'] == 'Asserted')])\n",
    "    asserted_fn = len(df[(df['label'] == 'undiscovered') & (df['true'] == 'Asserted')])\n",
    "    other_fp = len(df[(df['label'] == 'spurious') & (df['pred'] == 'Other')])\n",
    "    other_fn = len(df[(df['label'] == 'undiscovered') & (df['true'] == 'Other')])\n",
    "\n",
    "    tot_as_fp = asserted_fp + discovered_asserted_fp\n",
    "    tot_as_fn = asserted_fn + discovered_asserted_fn\n",
    "    asserted_precision = discovered_asserted_tp / (discovered_asserted_tp + tot_as_fp) if (discovered_asserted_tp + tot_as_fp) > 0 else 0.0\n",
    "    asserted_recall = discovered_asserted_tp / (discovered_asserted_tp + tot_as_fn) if (discovered_asserted_tp + tot_as_fn) > 0 else 0.0\n",
    "    asserted_f1 = (2*asserted_precision * asserted_recall) / (asserted_precision + asserted_recall) if (asserted_precision + asserted_recall) > 0 else 0.0\n",
    "    \n",
    "    tot_ot_fp = other_fp + discovered_other_fp\n",
    "    tot_ot_fn = other_fn + discovered_other_fn\n",
    "    other_precision = discovered_other_tp / (discovered_other_tp + tot_ot_fp) if (discovered_other_tp + tot_ot_fp) > 0 else 0.0\n",
    "    other_recall = discovered_other_tp / (discovered_other_tp + tot_ot_fn) if (discovered_other_tp + tot_ot_fn) > 0 else 0.0\n",
    "    other_f1 = (2*other_precision * other_recall) / (other_precision + other_recall) if (other_precision + other_recall) > 0 else 0.0\n",
    "\n",
    "    df.to_csv('../results/text2event_results.csv')\n",
    "    return {\n",
    "        \"trigger_precision\": trigger_precision,\n",
    "        \"trigger_recall\": trigger_recall,\n",
    "        \"trigger_f1\": trigger_f1,\n",
    "        \"discovered_other_recall\": discovered_other_recall,\n",
    "        \"discovered_other_precision\": discovered_other_precision,\n",
    "        \"discovered_other_f1\": discovered_other_f1,\n",
    "        \"discovered_asserted_recall\": discovered_asserted_recall,\n",
    "        \"discovered_asserted_precision\": discovered_asserted_precision,\n",
    "        \"discovered_asserted_f1\": discovered_asserted_f1,\n",
    "        \"asserted_precision\": asserted_precision,\n",
    "        \"asserted_recall\": asserted_recall,\n",
    "        \"asserted_f1\": asserted_f1,\n",
    "        \"other_precision\": other_precision,\n",
    "        \"other_recall\": other_recall,\n",
    "        \"other_f1\": other_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a261e552-f318-4ed0-9d19-3f4e56e6d53e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
