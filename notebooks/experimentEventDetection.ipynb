{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "410ddf7d-f6b8-410e-9f98-3b4e8740941a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 11:41:15.507464: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-10 11:41:15.645629: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-10 11:41:16.782054: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using the latest cached version of the module from /home/peder/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Mar  6 15:19:33 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from /home/peder/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--precision/155d3220d6cd4a6553f12da68eeb3d1f97cf431206304a4bc6e2d564c29502e9 (last modified on Fri Jan 24 10:47:30 2025) since it couldn't be found locally at evaluate-metric--precision, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from /home/peder/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--recall/11f90e583db35601050aed380d48e83202a896976b9608432fba9244fb447f24 (last modified on Fri Jan 24 10:47:31 2025) since it couldn't be found locally at evaluate-metric--recall, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from /home/peder/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/34c46321f42186df33a6260966e34a368f14868d9cc2ba47d142112e2800d233 (last modified on Fri Jan 24 10:47:32 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.abspath(os.getcwd()), \"..\"))\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorWithPadding, TrainingArguments, Trainer, AutoModel\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
    "\n",
    "from certainty import EventSentence, EventType, load_events, id2label, label2id, load_file, CACHE_DIR, GNNCertaintyPredictionModel, seed_everything, RANDOM_SEED\n",
    "\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "534b095e-b6e4-49cb-8865-fabe8f926bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_file('en_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "02fbfb51-4d79-42ff-9587-6015ea9db841",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train).drop_duplicates('text').drop_duplicates('events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aee1274e-3865-40af-b123-990b2cfda33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9e9649a4-e5e1-4740-8af5-7fd96f3b6ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_indices(span, offsets):\n",
    "    start_char, end_char = map(int, span.split(\":\"))\n",
    "    start_idx, end_idx = None, None\n",
    "\n",
    "    for i, (start, end) in enumerate(offsets):\n",
    "        if start <= start_char < end:  # Find first token in span\n",
    "            start_idx = i\n",
    "        if start < end_char <= end:  # Find last token in span\n",
    "            end_idx = i\n",
    "            break\n",
    "\n",
    "    return start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6376d10e-dfaf-40e8-9edf-890d5577644c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sent_id': 'bc/CNN_IP_20030329.1600.02/001',\n",
       "  'text': 'It was in northern Iraq today that an eight artillery round hit the site occupied by Kurdish fighters near Chamchamal',\n",
       "  'events': [{'event_type': 'Attack',\n",
       "    'event_polarity': 'Positive',\n",
       "    'event_genericity': 'Specific',\n",
       "    'event_modality': 'Asserted',\n",
       "    'trigger': [['hit'], ['60:63']],\n",
       "    'arguments': [[['northern Iraq'], ['10:23'], 'Place'],\n",
       "     [['today'], ['24:29'], 'Time-Within'],\n",
       "     [['an eight artillery round'], ['35:59'], 'Instrument'],\n",
       "     [['the site occupied by Kurdish fighters near Chamchamal'],\n",
       "      ['64:117'],\n",
       "      'Target']]}]},\n",
       " {'sent_id': 'bc/CNN_IP_20030329.1600.02/002',\n",
       "  'text': 'A day ago it was controlled by Iraqi troops and packed with these land mines but now we can drive far enough to see the outskirts of Kirkuk',\n",
       "  'events': [{'event_type': 'Transport',\n",
       "    'event_polarity': 'Positive',\n",
       "    'event_genericity': 'Specific',\n",
       "    'event_modality': 'Asserted',\n",
       "    'trigger': [['drive'], ['92:97']],\n",
       "    'arguments': [[['it'], ['10:12'], 'Destination'],\n",
       "     [['now'], ['81:84'], 'Time-Within'],\n",
       "     [['we'], ['85:87'], 'Artifact']]}]}]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "78d412c3-1dfc-4c04-b624-de760c6d3d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert/distilbert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR, local_files_only=True, trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "41a102eb-6efb-40bb-8b5d-d56418deff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"O\", 1: \"B-trigger\", 2: \"I-trigger\"}\n",
    "label2id = {label: idx for idx, label in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1e4b0e9c-56ad-412d-a0a9-5ee2dc8d732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded = []\n",
    "\n",
    "for sample in train:\n",
    "    toks = tokenizer(sample['text'], truncation=True, return_tensors=\"pt\", add_special_tokens=False, return_offsets_mapping=True, padding=\"max_length\")\n",
    "    offset_mapping=toks.pop(\"offset_mapping\")\n",
    "    \n",
    "    ner = ['O' for _ in range(0, len(toks['input_ids'][0]))]\n",
    "    \n",
    "    for i in range(0, len(sample['events'])):\n",
    "        trigger_span = sample['events'][i]['trigger'][1][0]\n",
    "        trigger_start, trigger_end = get_token_indices(trigger_span, offset_mapping[0])\n",
    "        try:\n",
    "            ner[trigger_start] = \"B-trigger\"\n",
    "        except:\n",
    "            print(trigger_start)\n",
    "            print(example[\"events\"])\n",
    "            print\n",
    "        for j in range(trigger_start+1, trigger_end+1):\n",
    "            ner[j] = \"I-trigger\"\n",
    "\n",
    "    # sample = {\"text\": sample[\"text\"], \"toks\": toks, \"ner_tags\": ner}\n",
    "    toks['labels'] = torch.tensor([label2id[label] for label in ner])\n",
    "    toks[\"input_ids\"] = toks[\"input_ids\"].squeeze(0)  # Convert from (seq_length,) → (1, seq_length)\n",
    "    toks[\"attention_mask\"] = toks[\"attention_mask\"].squeeze(0)  # Convert from (seq_length,) → (1, seq_length)\n",
    "    train_encoded.append(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d6ff22bf-bbe5-4791-a9a5-eefd79dc055d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 2009,  2001,  1999,  2642,  5712,  2651,  2008,  2019,  2809,  4893,\n",
       "         2461,  2718,  1996,  2609,  4548,  2011, 15553,  7299,  2379, 15775,\n",
       "        12458,  3511,  2389,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6d4689aa-2267-4e10-8379-a109611b43d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/peder/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--seqeval/541ae017dc683f85116597d48f621abc7b21b88dc42ec937c71af5415f0af63c (last modified on Thu Mar  7 10:47:46 2024) since it couldn't be found locally at evaluate-metric--seqeval, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0d6e76e6-fb92-40f5-9028-9b2c239b3e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_list = [\"O\", \"B-trigger\", \"I-trigger\"]\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5a89715e-a92b-43cf-81ed-04be699c393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_list(train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2368a483-1ab3-4d15-9a73-3031b2b0530c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, cache_dir=CACHE_DIR, local_files_only=True, num_labels=3, trust_remote_code=True, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8b4e4aa0-72f8-4899-a4ea-1e6b1763e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"../models/blabla\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")OUTPUT_DIR = \"../models/blabla\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "decd5714-2f85-4511-9b0a-a3c8ac78cf50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='406' max='406' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [406/406 11:19, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.011312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.997103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.007045</td>\n",
       "      <td>0.607581</td>\n",
       "      <td>0.394978</td>\n",
       "      <td>0.478737</td>\n",
       "      <td>0.997691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peder/.pyenv/versions/3.10.13/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=406, training_loss=0.024475010745043824, metrics={'train_runtime': 680.5212, 'train_samples_per_second': 9.54, 'train_steps_per_second': 0.597, 'total_flos': 848215261900800.0, 'train_loss': 0.024475010745043824, 'epoch': 2.0})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thread SenderThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/peder/.pyenv/versions/3.10.13/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/peder/.pyenv/versions/3.10.13/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 99, in _run\n",
      "    self._process(record)\n",
      "  File \"/home/peder/.pyenv/versions/3.10.13/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 327, in _process\n",
      "    self._sm.send(record)\n",
      "  File \"/home/peder/.pyenv/versions/3.10.13/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 385, in send\n",
      "    send_handler(record)\n",
      "  File \"/home/peder/.pyenv/versions/3.10.13/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 407, in send_request\n",
      "    send_handler(record)\n",
      "  File \"/home/peder/.pyenv/versions/3.10.13/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1147, in send_request_summary_record\n",
      "    self._update_summary_record(record.request.summary_record.summary)\n",
      "  File \"/home/peder/.pyenv/versions/3.10.13/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1141, in _update_summary_record\n",
      "    self._update_summary()\n",
      "  File \"/home/peder/.pyenv/versions/3.10.13/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1161, in _update_summary\n",
      "    with open(summary_path, \"w\") as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/peder/Skole/certainty/notebooks/wandb/run-20250310_114135-fm13gvnz/files/wandb-summary.json'\n",
      "wandb: ERROR Internal wandb error: file data was not synced\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c958e6-2d51-4990-9f3e-562e1c550cde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
