{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3091e00-4a5f-491c-81d0-cb61a0032c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3680e7d2-42c1-4932-8e33-aaa72cfb32ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of certainty failed: Traceback (most recent call last):\n",
      "  File \"/home/peder/.pyenv/versions/3.10.13/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/peder/.pyenv/versions/3.10.13/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/peder/.pyenv/versions/3.10.13/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/home/peder/Skole/certainty/notebooks/../certainty/__init__.py\", line 2, in <module>\n",
      "    from .utils import load_file, convert_events, bootstrap_metrics, id2label, label2id, load_events, logistic_data, create_bootstrap_figures, seed_everything, extract_events\n",
      "ImportError: cannot import name 'extract_events' from 'certainty.utils' (/home/peder/Skole/certainty/notebooks/../certainty/utils.py)\n",
      "]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'extract_events' from 'certainty' (/home/peder/Skole/certainty/notebooks/../certainty/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_args\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcertainty\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_file, CACHE_DIR, seed_everything, RANDOM_SEED, EventType, extract_events\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'extract_events' from 'certainty' (/home/peder/Skole/certainty/notebooks/../certainty/__init__.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.abspath(os.getcwd()), \"..\"))\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from typing import get_args\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from certainty import load_file, CACHE_DIR, seed_everything, RANDOM_SEED, EventType, extract_events\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63fe48fc-4410-4bee-8f03-1f62b0888957",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'extract_events' from 'certainty' (/home/peder/Skole/certainty/notebooks/../certainty/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcertainty\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extract_events\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'extract_events' from 'certainty' (/home/peder/Skole/certainty/notebooks/../certainty/__init__.py)"
     ]
    }
   ],
   "source": [
    "from certainty import extract_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f04c982-29ba-4228-9961-93a68794b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_indices(span, offsets):\n",
    "    start_char, end_char = map(int, span.split(\":\"))\n",
    "    start_idx, end_idx = None, None\n",
    "\n",
    "    for i, (start, end) in enumerate(offsets):\n",
    "        if start <= start_char < end:  # Find first token in span\n",
    "            start_idx = i\n",
    "        if start < end_char <= end:  # Find last token in span\n",
    "            end_idx = i\n",
    "            break\n",
    "\n",
    "    return start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1700a72-590c-43ba-9c62-f6388e9136da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(dataset, tokenizer, label2id):\n",
    "    encoded = []\n",
    "\n",
    "    for sample in dataset:\n",
    "        toks = tokenizer(sample['text'], truncation=True, return_tensors=\"pt\", add_special_tokens=False, return_offsets_mapping=True, padding=\"max_length\", max_length=254)\n",
    "        offset_mapping = toks.pop(\"offset_mapping\")\n",
    "\n",
    "        ner = ['O' for _ in range(0, len(toks['input_ids'][0]))]\n",
    "\n",
    "        for i in range(0, len(sample['events'])):\n",
    "            event = sample['events'][i]\n",
    "            trigger_span = event['trigger'][1][0]\n",
    "            t = event['event_type']\n",
    "            modality = event['event_modality']\n",
    "\n",
    "            trigger_start, trigger_end = get_token_indices(trigger_span, offset_mapping[0])\n",
    "\n",
    "            ner[trigger_start] = \"B-\" + modality\n",
    "            for j in range(trigger_start + 1, trigger_end + 1):\n",
    "                ner[j] = \"I-\" + modality\n",
    "\n",
    "        toks['labels'] = torch.tensor([label2id[label] for label in ner])\n",
    "        toks[\"input_ids\"] = toks[\"input_ids\"].squeeze(0)\n",
    "        toks[\"attention_mask\"] = toks[\"attention_mask\"].squeeze(0)\n",
    "        encoded.append(toks)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06948f8f-f4f4-489d-9999-7ddf0d64e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_file('en_train.json')\n",
    "train = pd.DataFrame(train).drop_duplicates('text').drop_duplicates('events').to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67287ad1-032f-4696-8e28-e902700a2ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sent_id': 'bc/CNN_IP_20030329.1600.02/001',\n",
       "  'text': 'It was in northern Iraq today that an eight artillery round hit the site occupied by Kurdish fighters near Chamchamal',\n",
       "  'events': [{'event_type': 'Attack',\n",
       "    'event_polarity': 'Positive',\n",
       "    'event_genericity': 'Specific',\n",
       "    'event_modality': 'Asserted',\n",
       "    'trigger': [['hit'], ['60:63']],\n",
       "    'arguments': [[['northern Iraq'], ['10:23'], 'Place'],\n",
       "     [['today'], ['24:29'], 'Time-Within'],\n",
       "     [['an eight artillery round'], ['35:59'], 'Instrument'],\n",
       "     [['the site occupied by Kurdish fighters near Chamchamal'],\n",
       "      ['64:117'],\n",
       "      'Target']]}]},\n",
       " {'sent_id': 'bc/CNN_IP_20030329.1600.02/002',\n",
       "  'text': 'A day ago it was controlled by Iraqi troops and packed with these land mines but now we can drive far enough to see the outskirts of Kirkuk',\n",
       "  'events': [{'event_type': 'Transport',\n",
       "    'event_polarity': 'Positive',\n",
       "    'event_genericity': 'Specific',\n",
       "    'event_modality': 'Asserted',\n",
       "    'trigger': [['drive'], ['92:97']],\n",
       "    'arguments': [[['it'], ['10:12'], 'Destination'],\n",
       "     [['now'], ['81:84'], 'Time-Within'],\n",
       "     [['we'], ['85:87'], 'Artifact']]}]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9599732d-b2fe-408d-94ea-3d43ebf3502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased', cache_dir=CACHE_DIR, local_files_only=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b321f0f6-e951-4f1b-8c1e-d0cfd19c0977",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "id2label = {}\n",
    "i = 0\n",
    "id2label[i] = \"O\"\n",
    "label_list.append(\"O\")\n",
    "i += 1\n",
    "id2label[i] = \"B-Asserted\"\n",
    "label_list.append(\"B-Asserted\")\n",
    "id2label[i + 1] = \"I-Asserted\"\n",
    "label_list.append(\"I-Asserted\")\n",
    "id2label[i + 2] = \"B-Other\"\n",
    "label_list.append(\"B-Other\")\n",
    "id2label[i + 3] = \"I-Other\"\n",
    "label_list.append(\"I-Other\")\n",
    "label2id = {label: idx for idx, label in id2label.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d40bfcdd-2262-4983-8bf2-842ba6d08efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-Asserted', 'I-Asserted', 'B-Other', 'I-Other']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8228d24a-8dc3-4758-8808-8c529ff2f522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O', 1: 'B-Asserted', 2: 'I-Asserted', 3: 'B-Other', 4: 'I-Other'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89229d97-6ca7-4eb5-8a62-7cbdebcd57b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0, 'B-Asserted': 1, 'I-Asserted': 2, 'B-Other': 3, 'I-Other': 4}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fac32269-04a8-4dd3-85e3-6a8b031de865",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded = encode_dataset(train, tokenizer, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13538fb3-a8ce-422f-be70-65574c5d9c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 2008,  1005,  1055,  2138,  6056,  4959,  9924, 16405, 29033,  2098,\n",
       "         2023,  8956,  2597,  2006,  1996,  4564,  2682, 15775, 12458,  3511,\n",
       "         2389,  1998,  8956,  3629,  2081,  1037, 27151,  7822,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4785159-52cc-40e7-9d64-9026ef086875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    'distilbert/distilbert-base-uncased', cache_dir=CACHE_DIR, local_files_only=True, num_labels=len(id2label.items()), trust_remote_code=True, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b128a36-f7a1-47ed-a73a-2eca9e84bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list(train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45350d69-e2ab-48b9-93e8-12077ae0870f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peder/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeder-august\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/peder/Skole/certainty/notebooks/wandb/run-20250319_094428-axtfg08x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peder-august/huggingface/runs/axtfg08x' target=\"_blank\">../models/blabla</a></strong> to <a href='https://wandb.ai/peder-august/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peder-august/huggingface' target=\"_blank\">https://wandb.ai/peder-august/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peder-august/huggingface/runs/axtfg08x' target=\"_blank\">https://wandb.ai/peder-august/huggingface/runs/axtfg08x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='204' max='203' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [203/203 15:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Trainer: evaluation requires an eval_dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 20\u001b[0m\n\u001b[1;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      4\u001b[0m         output_dir\u001b[38;5;241m=\u001b[39mOUTPUT_DIR,\n\u001b[1;32m      5\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m         use_cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     16\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     17\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/trainer.py:2487\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2484\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2487\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2490\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   2491\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/trainer.py:2915\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2913\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2915\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   2918\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/trainer.py:2872\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2871\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 2872\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2873\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2875\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/trainer.py:3861\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3858\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m   3859\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m-> 3861\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_eval_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[1;32m   3863\u001b[0m     eval_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(eval_dataloader)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/trainer.py:965\u001b[0m, in \u001b[0;36mTrainer.get_eval_dataloader\u001b[0;34m(self, eval_dataset)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;124;03mReturns the evaluation [`~torch.utils.data.DataLoader`].\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;124;03m        If a `str`, will use `self.eval_dataset[eval_dataset]` as the evaluation dataset. If a `Dataset`, will override `self.eval_dataset` and must implement `__len__`. If it is a [`~datasets.Dataset`], columns not accepted by the `model.forward()` method are automatically removed.\u001b[39;00m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer: evaluation requires an eval_dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    967\u001b[0m \u001b[38;5;66;03m# If we have persistent workers, don't do a fork bomb especially as eval datasets\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;66;03m# don't change during training\u001b[39;00m\n\u001b[1;32m    969\u001b[0m dataloader_key \u001b[38;5;241m=\u001b[39m eval_dataset \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Trainer: evaluation requires an eval_dataset."
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = \"../models/blabla\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=1,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        use_cpu=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c3ce51db-26d7-4592-a3ba-9f788b7fce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c4a78fe0-cd2a-4180-b103-4023f37acabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_encoded, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d815425b-4ba4-4781-9c94-e696de2f46a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "05c51e1f-ccd3-4655-88b4-5adc24196eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# Storage for all predictions and labels\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    for batch in train_dataloader:  # Assuming you're using a DataLoader\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        # Get logits for token classification\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Convert logits to predictions (take the argmax)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Get true labels\n",
    "        labels = batch[\"labels\"]\n",
    "        \n",
    "        # Convert to lists and append to storage\n",
    "        # Only consider tokens that are not padding (attention_mask = 1)\n",
    "        for i, mask in enumerate(batch[\"attention_mask\"]):\n",
    "            pred = predictions[i][mask == 1].cpu().tolist()\n",
    "            true = labels[i][mask == 1].cpu().tolist()\n",
    "            \n",
    "            all_pred_labels.extend(pred)\n",
    "            all_true_labels.extend(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "04ef081c-0124-4e34-b66b-32ed0b05abed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86082"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c960c052-a046-4ef4-be41-925730ff9c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86082"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "733b880d-b5c9-4695-8edf-f53a6e11e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_events(labels):\n",
    "    events = []\n",
    "    current_event = None\n",
    "\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label == 0:  # O tag\n",
    "            if current_event:\n",
    "                events.append((current_event[0], idx - 1, current_event[1], current_event[2]))\n",
    "                current_event = None\n",
    "\n",
    "        elif label == 1:  # B-Asserted\n",
    "            if current_event:\n",
    "                events.append((current_event[0], idx - 1, current_event[1], current_event[2]))\n",
    "            current_event = (idx, \"event\", True)  # True for Asserted\n",
    "\n",
    "        elif label == 2:  # I-Asserted\n",
    "            if not current_event or current_event[2] != True:\n",
    "                current_event = (idx, \"event\", True)\n",
    "\n",
    "        elif label == 3:  # B-Other\n",
    "            if current_event:\n",
    "                events.append((current_event[0], idx - 1, current_event[1], current_event[2]))\n",
    "            current_event = (idx, \"event\", False)  # False for Other\n",
    "\n",
    "        elif label == 4:  # I-Other\n",
    "            if not current_event or current_event[2] != False:\n",
    "                current_event = (idx, \"event\", False)\n",
    "\n",
    "    if current_event:\n",
    "        events.append((current_event[0], len(labels) - 1, current_event[1], current_event[2]))\n",
    "\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "68439f93-e773-44c3-a820-2534cd575be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_events = extract_events(all_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "247de59b-4133-48ad-bc4a-b65a34684c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_events = extract_events(all_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f852def-36d0-4724-a816-bf39e5affa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_events(true_events, pred_events):\n",
    "    discovered = []\n",
    "    undiscovered = true_events.copy()\n",
    "    spurious = pred_events.copy()\n",
    "\n",
    "    for pred_event in pred_events[:]:\n",
    "        for true_event in true_events[:]:\n",
    "            if pred_event[0] == true_event[0] and pred_event[1] == true_event[1]:\n",
    "                discovered.append((true_event, pred_event))\n",
    "                if true_event in undiscovered:\n",
    "                    undiscovered.remove(true_event)\n",
    "                if pred_event in spurious:\n",
    "                    spurious.remove(pred_event)\n",
    "\n",
    "    return discovered, undiscovered, spurious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b5ba76dc-e090-4c13-ae1e-a9e7ad08f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "discovered, undiscovered, spurious = match_events(true_events, pred_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4335d368-0293-4aac-972a-96bc96d42ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, 15, 'event', True),\n",
       " (16, 16, 'event', True),\n",
       " (58, 58, 'event', False),\n",
       " (70, 70, 'event', True),\n",
       " (140, 140, 'event', True),\n",
       " (174, 175, 'event', True),\n",
       " (182, 183, 'event', False),\n",
       " (197, 199, 'event', True),\n",
       " (230, 232, 'event', False),\n",
       " (260, 260, 'event', True)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "undiscovered[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "44fb59e5-2607-489a-9c5c-4b047aa9c4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(174, 174, 'event', True),\n",
       " (175, 175, 'event', True),\n",
       " (182, 182, 'event', True),\n",
       " (183, 183, 'event', True),\n",
       " (278, 278, 'event', True),\n",
       " (410, 410, 'event', True),\n",
       " (522, 522, 'event', True),\n",
       " (530, 530, 'event', True),\n",
       " (620, 620, 'event', True),\n",
       " (763, 763, 'event', True)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spurious[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82dacd1a-e0e6-492a-9b5a-a33b86c99f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_event_metrics(discovered, undiscovered, spurious):\n",
    "    true_positives = len(discovered)\n",
    "    false_negatives = len(undiscovered)\n",
    "    false_positives = len(spurious)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"event_precision\": precision,\n",
    "        \"event_recall\": recall,\n",
    "        \"event_f1\": f1,\n",
    "        \"true_positives\": true_positives,\n",
    "        \"false_negatives\": false_negatives,\n",
    "        \"false_positives\": false_positives\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1ac2a052-31c2-47dd-8104-a1e410c9b462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'event_precision': 0.7431127012522362,\n",
       " 'event_recall': 0.4874442619103497,\n",
       " 'event_f1': 0.588718820861678,\n",
       " 'true_positives': 2077,\n",
       " 'false_negatives': 2184,\n",
       " 'false_positives': 718}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_event_metrics(discovered, undiscovered, spurious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b490005-20d7-45cf-ace9-28b07cbc7a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_factuality_metrics_discovered(discovered, spurious):\n",
    "    true_asserted = sum(1 for true_event, pred_event in discovered\n",
    "                        if true_event[3] and pred_event[3])\n",
    "\n",
    "    true_other = sum(1 for true_event, pred_event in discovered\n",
    "                     if not true_event[3] and not pred_event[3])\n",
    "\n",
    "    false_asserted = sum(1 for true_event, pred_event in discovered\n",
    "                         if not true_event[3] and pred_event[3])\n",
    "\n",
    "    false_other = sum(1 for true_event, pred_event in discovered\n",
    "                      if true_event[3] and not pred_event[3])\n",
    "\n",
    "    asserted_precision = true_asserted / (true_asserted + false_asserted) if (true_asserted + false_asserted) > 0 else 0\n",
    "    asserted_recall = true_asserted / (true_asserted + false_other) if (true_asserted + false_other) > 0 else 0\n",
    "    asserted_f1 = 2 * asserted_precision * asserted_recall / (asserted_precision + asserted_recall) if (asserted_precision + asserted_recall) > 0 else 0\n",
    "\n",
    "    other_precision = true_other / (true_other + false_other) if (true_other + false_other) > 0 else 0\n",
    "    other_recall = true_other / (true_other + false_asserted) if (true_other + false_asserted) > 0 else 0\n",
    "    other_f1 = 2 * other_precision * other_recall / (other_precision + other_recall) if (other_precision + other_recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"discovered_true_asserted\": true_asserted,\n",
    "        \"discovered_true_other\": true_other,\n",
    "        \"discovered_false_asserted\": false_asserted,\n",
    "        \"discovered_false_other\": false_other,\n",
    "        \"discovered_asserted_precision\": asserted_precision,\n",
    "        \"discovered_asserted_recall\": asserted_recall,\n",
    "        \"discovered_asserted_f1\": asserted_f1,\n",
    "        \"discovered_other_precision\": other_precision,\n",
    "        \"discovered_other_recall\": other_recall,\n",
    "        \"discovered_other_f1\": other_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5d265373-0bb0-423c-aaf8-259fb41861b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'discovered_true_asserted': 1693,\n",
       " 'discovered_true_other': 0,\n",
       " 'discovered_false_asserted': 384,\n",
       " 'discovered_false_other': 0,\n",
       " 'discovered_asserted_precision': 0.8151179585941262,\n",
       " 'discovered_asserted_recall': 1.0,\n",
       " 'discovered_asserted_f1': 0.8981432360742706,\n",
       " 'discovered_other_precision': 0,\n",
       " 'discovered_other_recall': 0.0,\n",
       " 'discovered_other_f1': 0}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_factuality_metrics_discovered(discovered, spurious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1cfa3ff7-4c05-44ed-8459-ccdc3dfd9198",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_factuality_metrics_all(discovered, undiscovered, spurious):\n",
    "    true_asserted = sum(1 for true_event, pred_event in discovered\n",
    "                        if true_event[3] and pred_event[3])\n",
    "\n",
    "    true_other = sum(1 for true_event, pred_event in discovered\n",
    "                     if not true_event[3] and not pred_event[3])\n",
    "\n",
    "    false_asserted = sum(1 for true_event, pred_event in discovered\n",
    "                         if not true_event[3] and pred_event[3])\n",
    "\n",
    "    false_other = sum(1 for true_event, pred_event in discovered\n",
    "                      if true_event[3] and not pred_event[3])\n",
    "\n",
    "    false_negative_asserted = sum(1 for event in undiscovered if event[3])  # Asserted events that weren't discovered\n",
    "    false_negative_other = sum(1 for event in undiscovered if not event[3])  # Other events that weren't discovered\n",
    "\n",
    "    false_positive_asserted = sum(1 for event in spurious if event[3])  # Spurious events predicted as Asserted\n",
    "    false_positive_other = sum(1 for event in spurious if not event[3])  # Spurious events predicted as Other\n",
    "\n",
    "    all_tp_asserted = true_asserted\n",
    "    all_tp_other = true_other\n",
    "    all_fp_asserted = false_asserted + false_positive_asserted\n",
    "    all_fp_other = false_other + false_positive_other\n",
    "    all_fn_asserted = false_other + false_negative_asserted\n",
    "    all_fn_other = false_asserted + false_negative_other\n",
    "\n",
    "    asserted_precision = all_tp_asserted / (all_tp_asserted + all_fp_asserted) if (all_tp_asserted + all_fp_asserted) > 0 else 0\n",
    "    asserted_recall = all_tp_asserted / (all_tp_asserted + all_fn_asserted) if (all_tp_asserted + all_fn_asserted) > 0 else 0\n",
    "    asserted_f1 = 2 * asserted_precision * asserted_recall / (asserted_precision + asserted_recall) if (asserted_precision + asserted_recall) > 0 else 0\n",
    "\n",
    "    other_precision = all_tp_other / (all_tp_other + all_fp_other) if (all_tp_other + all_fp_other) > 0 else 0\n",
    "    other_recall = all_tp_other / (all_tp_other + all_fn_other) if (all_tp_other + all_fn_other) > 0 else 0\n",
    "    other_f1 = 2 * other_precision * other_recall / (other_precision + other_recall) if (other_precision + other_recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"all_true_positive_asserted\": all_tp_asserted,\n",
    "        \"all_true_positive_other\": all_tp_other,\n",
    "        \"all_false_positive_asserted\": all_fp_asserted,\n",
    "        \"all_false_positive_other\": all_fp_other,\n",
    "        \"all_false_negative_asserted\": all_fn_asserted,\n",
    "        \"all_false_negative_other\": all_fn_other,\n",
    "        \"all_asserted_precision\": asserted_precision,\n",
    "        \"all_asserted_recall\": asserted_recall,\n",
    "        \"all_asserted_f1\": asserted_f1,\n",
    "        \"all_other_precision\": other_precision,\n",
    "        \"all_other_recall\": other_recall,\n",
    "        \"all_other_f1\": other_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "69776c7d-326a-42b4-99f9-a2e619bc216f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all_true_positive_asserted': 1693,\n",
       " 'all_true_positive_other': 0,\n",
       " 'all_false_positive_asserted': 1102,\n",
       " 'all_false_positive_other': 0,\n",
       " 'all_false_negative_asserted': 1523,\n",
       " 'all_false_negative_other': 1045,\n",
       " 'all_asserted_precision': 0.6057245080500895,\n",
       " 'all_asserted_recall': 0.5264303482587065,\n",
       " 'all_asserted_f1': 0.56330061553818,\n",
       " 'all_other_precision': 0,\n",
       " 'all_other_recall': 0.0,\n",
       " 'all_other_f1': 0}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_factuality_metrics_all(discovered, undiscovered, spurious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c7c3db-10c6-492a-81c3-61128734d5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
